Van mens naar data tot doelwit van kamikazedrones Algoritmische oorlogsvoering

Als u voor deze zoekvraag nog andere filters hebt geselecteerd, worden die door deze selectie vervangen. Klik op "Doorgaan" om de huidige filters te vervangen of klik op "Annuleren" om ze te behouden.
Citatie exporteren
De Groene Amsterdammer
25 juli 2024
Kunstmatige intelligentie zou ervoor zorgen dat er tijdens oorlogen minder burgerdoden vallen. In realiteit vallen er juist meer. Want waar mensen worden gereduceerd tot datapunten, voelt vuren al snel als objectief en correct.
Op 23 oktober 2023verloor de elfjarige Dareen haar vader, moeder, broer, oma, opa en 65 andere familieleden door een Israëlisch bombardement in Gaza. In een interview met de nos vertelt ze dat haar familie bijeen was gekomen in haar ouderlijk huis toen ze werden gebombardeerd. Dareen en haar broertje overleefden als enigen het bombardement. Na de aanval waren haar verwondingen zo ernstig dat ze twee weken in coma lag en vier maanden in het ziekenhuis moest blijven. Vanuit een rolstoel in Qatar vertelt zij nu haar verhaal, waarbij zij een voor een de foto's van haar omgekomen familieleden laat zien.
Hoewel we het nooit zeker zullen weten, is de kans aanwezig dat het huis van Dareen en haar familie niet door een mens, maar door een AI-gedreven systeem als doelwit werd geïdentificeerd. Volgens recente onthullingen door een groep Palestijns-Israëlische journalisten in+972 MagazineenLocal Callmaakt het Israëlische leger namelijk op grote schaal gebruik van algoritmen en artificiële intelligentie (AI) om potentiële doelwitten te identificeren. De journalisten beschrijven op basis van interviews met betrokkenen binnen het Israëlische Defensieleger (idf) drie van zulke AI-systemen, genaamd Lavendel, Evangelie en Where's Daddy?
Op basis van grote hoeveelheden data voorspelt Lavendel welke personen er bij de militaire tak van Hamas horen, identificeert Evangelie de gebouwen waar deze mensen wonen en werken, en berekent Where's Daddy? vervolgens wanneer deze persoon thuis komt, zodat die daar kan worden aangevallen. Volgens Israelische bronnen selecteerden deze AI-systemen samen meer dan 37.000 Palestijnen als mogelijk doelwit. Met behulp van AI kan het idf nu dagelijks honderd gebouwen identificeren, terwijl het vóór deze oorlog om vijftig doelwitten per jaar ging. Militaire gebruikers krijgen daarbij naar verluidt slechts twintig seconden de tijd om te controleren of een voorgesteld doelwit legitiem is. Dit roept grote twijfels op over de mate waarin militairen nog in staat zijn de beslissingen van deze AI-systemen te begrijpen en kritisch te controleren.
Daarmee leidt het gebruik van AI tot een enorme toename van het aantal luchtaanvallen en draagt het bij aan de verwoesting en menselijke tol in Gaza. De realiteit in Gaza weerlegt het veelgehoorde argument van defensiespecialisten dat AI een rol kan spelen in het beperken van burgerslachtoffers. De werkelijkheid is een andere: het gebruik van AI leidt niet tot 'schone' en 'precieze' oorlogen met een beperkt aantal burgerslachtoffers, maar zorgt er juist voor dat er op veel grotere schaal kan worden aangevallen doordat de technologie veel meer en veel sneller doelwitten genereert dan mensen kunnen. Met veel meertarget-profielen en onder grote politieke en militaire druk om snel te reageren, hebben de betrokken militairen weinig tot geen zicht op - en controle over - de antwoorden en suggesties die AI genereert. Dit leidt tot meer en ook nieuwe vormen van geweld en burgerleed, waarbij mensen tot 'datapunten' worden gereduceerd en op basis van probabilistische modellen kunnen worden gedood.
Tegelijkertijd hebben mensen de onbewuste neiging om antwoorden en aanbevelingen die een computer genereert als objectief en correct te beschouwen. Zo wordt de verantwoordelijkheid voor burgerleed van de mens naar de technologie gedelegeerd. In een context waarin militairen toch al steeds afhankelijker worden van de data, algoritmen en platformen van technologiebedrijven, leidt dit tot een verdere verwatering van verantwoordelijkheid en aansprakelijkheid in militaire operaties. Zonder voldoende democratische controle zal dit het oorlogsgeweld verder opdrijven met meer burgerslachtoffers tot gevolg.
AI en het militaire domeinzijn historisch gezien nauw verweven. De opkomst van AI als wetenschappelijke discipline was in de jaren vijftig en zestig sterk afhankelijk van financiering van darpa, het Amerikaanse onderzoeksinstituut voor militaire technologie. Veel van de recente ontwikkelingen en doorbraken in AI zijn een direct gevolg van onderzoeksfinanciering door darpa, zoals beeldherkenning, zelfrijdende auto's en computervertaling.
Maar de militaire steun van darpa had ook indirecte gevolgen. AI-onderzoeker Kate Crawford beschrijft in haar boekAtlas of AIhoe het wetenschappelijke veld van AI vanaf het begin sterk werd beïnvloed door de prioriteiten van het Amerikaanse leger, waarondercommand and control,surveillanceen automatisering; en hoe militaire denkkaders en concepten alstargetenassetde gemeenschappelijke taal voor AI-onderzoek vormen.
AI wordt ook al veel langer toegepast in allerlei wapens. Dit geldt voor luchtafweersystemen als de Israëlische Iron Dome, die op machinesnelheid moeten kunnen beslissen of een inkomend projectiel vijandig is en uit de lucht moet worden gehaald, maar ook voor wapens als de kamikazedrone, die zich op een doelwit stort en daarbij zelf verloren gaat. Deze wapens worden nu op grote schaal ingezet in de oorlog in Oekraïne, maar werden al in de jaren tachtig ontwikkeld.
In de jaren zestig en zeventig experimenteerden de Amerikanen ook al volop met elektronische oorlogsvoering. In de Vietnamoorlog dropte de Amerikaanse luchtmacht tijdens de geheime operatie Igloo White duizenden sensoren langs de Ho Chi Minh-route in een poging de bewegingen van de vijand in kaart te brengen met hun eerste computers. Hoewel die operatie achteraf gezien vooral erg duur en weinig succesvol was, legde ze wel de basis voor de hoogtechnologische en datagedreven oorlogsvoering van de 21ste eeuw.
De laatste jaren gaan investeringen in AI, onbemande systemen en datagedreven oorlogsvoering gepaard met een algemene afname van het aantal soldaten dat zich aan de frontlinie bevindt. Deze beweging wordt gedreven door de politieke noodzaak voor democratieën om een 'risicovrije' oorlog te voeren en het aantal slachtoffers in de eigen gelederen te beperken. Daarbij leunen legers steeds zwaarder op wijdverspreide surveillance en data van onder andere dronecamera's, satellietbeelden, telefoons en beveiligingscamera's om kennis over het slagveld te vergaren. In de door Amerika geleide coalitieoorlogen in Irak, Afghanistan en Syrië vormden deze data de basis voor zogenaamdesignature strikes.
Bij dit type aanval selecteren inlichtingenofficieren doelwitten niet op basis van de identiteit van een persoon, maar kijken ze naar verdachte gedragspatronen die ze in de metadata van bijvoorbeeld telefoonverkeer herkennen. Ondanks de claims van precisie leidden deze signature strikes tot grote aantallen misidentificaties en burgerslachtoffers. Zo zegt een Amerikaanse dronevlieger over zijn werk in Jemen het volgende: 'Eigenlijk vuren we alleen op een mobiele telefoon', in de hoop daarmee de tegenstander te raken. '[Maar] als de bom valt, weet je alleen dat de telefoon geraakt is - niet wie hem vast had.' Voor militairen zelf resulteerde deze data-explosie in een veel banaler probleem. Er werd nu zoveel informatie vergaard, dat analisten 'zwommen in sensoren en verdronken in data' en men op zoek moest naar nieuwe dataanalysemethoden om die gegevens te kunnen blijven verwerken en gebruiken.
AI wordt aangedragen als oplossing voor dit probleem. De commerciële wereld leert ons dat je met algoritmen enmachine learninggrote hoeveelheden data kunt verwerken tot bruikbare informatie. De afgelopen jaren zochten overheden wereldwijd - met Amerika en China voorop - daarom gretig naar nieuwe samenwerkingen met de technologiesector. Diezelfde sector profiteert ondertussen volop van de AI-boom, in combinatie met de recente toename in defensie-uitgaven.
Uit onderzoek van de Amerikaanse non-profitorganisatie Tech Inquiry blijkt bijvoorbeeld dat de vijf grootste defensiecontracten in Amerika in de afgelopen jaren naar tech-giganten Google, Microsoft en Amazon gingen. Daarnaast sloten Google en Amazon in 2021 een omvangrijk contract met de Israëlische overheid voor de levering van AI- en clouddiensten. Dit zogeheten Project Nimbus kwam de afgelopen maanden onder vuur te liggen, omdat het de digitale omgeving biedt voor de uitrol van gezichtsherkenningstechnologie, 'sentimentanalyse' en objecttracking door de Israëlische autoriteiten.
Naast een groeiende rol voor Big Techbedrijven zien we ook dat veel andere commerciële bedrijven in de oorlogsindustrie stappen. Neem het Amerikaanse bedrijf Palantir, in 2003 opgericht door PayPal-miljardair en durfkapitalist Peter Thiel, dat zich specialiseert in software voor dataverzameling voor politie en grensbewaking, maar ook voor defensie. Thiels software zou hebben geholpen bij de opsporing van Osama Bin Laden in 2011. Zijn bedrijf was ook betrokken bij de jarenlange internationale en illegale dataverzamelingspraktijken door de Amerikaanse inlichtingendienst die klokkenluider Edward Snowden in 2013 aan het licht bracht.
Een andere belangrijke nieuwe speler is het Amerikaanse bedrijf Anduril, met aan het roer de excentrieke Palmer Luckey, die in een recent interview met deFinancial Timeszei te streven naar een wereld waarin een technologisch superieur Amerika zich 'een volwassene waant in een kamer vol met dictatoriale kleuters'. Deze nieuwe partnerschappen tussen defensie, de traditionele wapenindustrie en de technologiesector drijven de investeringen in militaire toepassingen van AI steeds verder op.
Tegelijkertijd waarschuwen wetenschappers en ngo's al langer voor de risico's die met het gebruik van militaire AI gepaard gaan. Dan gaat het vooral over de ontwikkeling van AI voor wapensystemen die het selecteren en aanvallen van individuele doelen zonder menselijke betrokkenheid mogelijk maken. Volgens een groot deel van de internationale gemeenschap - waaronder Nederland - zijn deze wapens niet in staat om de kernregels van het internationaal humanitair recht na te leven en moet er daarom een verbod komen.
Ook worden vaak fundamentele ethische overwegingen aangehaald, zoals dat de beslissing over leven en dood nooit door een machine gemaakt zou moeten worden. Volgens deskundigen liggen er oplossingen in nieuwe internationale regelgeving rond de taakverdeling tussen mens en machine en het inbouwen van garanties dat bij het gebruik van deze wapens altijd een vorm van 'betekenisvolle' menselijke controle plaatsvindt. De concrete uitwerking van die regels en de naleving ervan is echter zeer onwaarschijnlijk in de huidige geopolitieke context van voortdurende rivaliteit en wantrouwen tussen Amerika, China en Rusland.
Ondertussen vormen de frontliniesin Oekraïne en Gaza voor veel overheden en bedrijven een proeftuin waarin ze hun nieuwe technologieën kunnen testen. Dit geldt voor de Amerikanen die Oekraïne openlijk beschrijven als een 'laboratorium' voor de ontwikkeling van militaire AI en andere moderne wapens. Volgens Palantir-ceo Alex Karp is zijn bedrijf op dit moment betrokken bij het overgrote deel van de militaire besluitvorming in Oekraïne, waaronder beslissingen over leven en dood op het slagveld.
Maar ook voor Nederland is Oekraïne een belangrijke proeftuin. In maart dit jaar bracht de toen demissionaire minister van Defensie Kajsa Ollongren een bezoek aan Kyiv, met in haar kielzog de vertegenwoordigers van drie Nederlandse start-upbedrijven. Die sloten daar contracten af met Oekraïense partners. Ollongren noemde dit een 'win-winsituatie'. Zo maakt het Nederlandse bedrijf Avalor AI, dat software ontwikkelt om drones zelfstandig in een zwerm te kunnen laten vliegen, nu gebruik van de ervaringen en inzichten van Oekraïense soldaten op het slagveld om hun product te verbeteren.
Voortgestuwd door de inzichten aan het front experimenteren vrijwel alle grote en middelgrote militaire organisaties met het gebruik van AI en autonomie in wapens. De gangbare term 'autonome wapensystemen' suggereert dat het hier om een nieuwe en aparte categorie gaat, maar dat is misleidend. AI wordt op verschillende manieren en voor allerlei functies in wapensystemen toegepast en de ontwikkeling gaat snel. Het onderscheid tussen 'semi-autonome' en 'volledig autonome' wapens waar de Nederlandse regering aan vasthoudt op advies van de Adviesraad Internationale Vraagstukken (aiv) bestaat in werkelijkheid niet, of alleen in functionele zin.
Volgens deskundigen maken Oekraïense en Russische troepen op dit moment bijvoorbeeld al volop gebruik van drones die volledig autonoom kunnen opereren en doelen aanvallen zonder controle of betrokkenheid van de mens. Een militaire gebruiker kan immers met slechts een druk op de knop overschakelen van semi-autonome naar volledig autonome functionaliteit. Omdat in moderne oorlogssituaties beslissingen onder steeds grotere tijdsdruk moeten worden genomen, is de kans groot dat de keuze voor autonomie sneller en steeds vaker wordt gemaakt. Daarbij is het voor buitenstaanders onmogelijk te verifiëren of een aanval door een mens, semi-autonoom of volledig autonoom wapen is uitgevoerd.
Een andere cruciale toepassing van AI waar nu volop mee wordt geëxperimenteerd, heeft betrekking op het automatiseren van de selectie van nieuwe doelwitten. Lavendel, Evangelie en Where's Daddy? vallen onder die categorie, maar dit soort systemen beperken zich niet tot Israël of de huidige oorlog in Gaza. Zo experimenteren de Amerikanen onder de naam Project Maven al sinds 2017 met AI-gedreven doelwitselectie in Jemen, Syrië, Irak en meer recentelijk ook in Oekraïne.
Deze AI-gestuurde systemen werken op basis van waarschijnlijkheid en correlatie. In Gaza zijn ze getraind om kenmerken te herkennen waarvan wordt aangenomen dat ze representatief zijn voor mensen die banden hebben met de militaire tak van Hamas. Het gaat dan bijvoorbeeld om lidmaatschap van dezelfde WhatsApp-groep als een bekende Hamas-militant, of het regelmatig veranderen van mobiele telefoon of adres. Vervolgens analyseren de systemen de datagegevens van de 2,3 miljoen inwoners van Gaza die Israel al jaren op grote schaal verzamelt. Op basis van de vooraf bepaalde kenmerken voorspellen ze de waarschijnlijkheid dat een persoon lid is van Hamas (Lavendel), waar die woont of werkt (Evangelie) en of die aanwezig is op dat adres (Where's Daddy?).
Hoewel in dit geval de keuze om een doelwit aan te vallen nog steeds bij een menselijke gebruiker ligt, is de beslissingsruimte zeer beperkt. De inlichtingenofficieren die Evangelie gebruiken, verklaarden hoe het AI-systeem hen hielp om van 'vijftig doelwitten per jaar' naar 'honderd doelwitten per dag' te gaan - en dat Lavendel in totaal meer dan '37.000 mensen als mogelijk doelwit identificeerde'. Ze reflecteerden ook op hoe het gebruik van AI hun eigen aandeel in de besluitvorming sterk reduceerde: 'In dit stadium investeerde ik in elk doelwit twintig seconden. (...) Ik had als mens geen enkele toegevoegde waarde (...). Het bespaarde veel tijd.'
Op deze manier leidt AI niet alleen tot de automatisering van doelwitselectie, maar ook tot een systematisering van kritieke beslissingen over leven en dood, zonder betekenisvolle menselijke controle. In tegenstelling tot wat voorstanders van AI-gedreven 'precisie'-optreden vaak beweren, toont de oorlog in Gaza aan dat deze systematisering tot meer burgerdoden en meer burgerschade leidt.
Bijna dagelijks berichtenonze media over burgerdoden in Gaza. Binnen een maand na de start van de oorlog vielen er 10.022 doden, binnen drie maanden 23.469, en na zes maanden ging het om een onvoorstelbaar aantal van 33.207 slachtoffers. Deskundigen spreken inmiddels over een van de meest dodelijke en meedogenloze conflicten van de 21ste eeuw.
Maar hoe belangrijk focussen op aantallen burgerdoden in verslaggeving ook is, het heeft een keerzijde. Cijfers objectiveren slachtoffers tot op zekere hoogte op dezelfde wijze als AI-gestuurde selectie- en targetingprocessen dat doen. Mensenlevens worden gereduceerd tot iets wat gemeten, geteld en gecategoriseerd kan worden als nevenschade. Daarom is het tellen van burgerdoden slechts het begin van een verhaal over de impact van oorlog op burgers. Een volledige analyse van de gevolgen van acht maanden AI-gedreven aanvallen in Gaza omvat de directe vormen van leed, zoals de 36.096 doden, 81.136 gewonden en de grootschalige verwoesting - inmiddels meer dan zestig procent - van alle woonhuizen. Maar het omvat ook indirectere vormen van leed, waaronder het feit dat 1,7 miljoen mensen op de vlucht zijn en moeten overleven zonder basisvoorzieningen als schoon drinkwater, voedsel, elektriciteit en medicijnen.
Alleen als we de gevolgen van oorlogsvoering in hun veelzijdigheid beschouwen, begrijpen we hoe verschillende vormen van burgerleed elkaar beïnvloeden en versterken. Neem het verhaal dat Shorouk Al Rantisi in januari 2024 aan Artsen Zonder Grenzen vertelde. Toen de oorlog begon, vluchtte zij met haar jonge gezin naar het vluchtelingenkamp Jabalia in Noord-Gaza, dat kort na hun aankomst door het Israëlische leger onder vuur werd genomen. Naar verluidt kwamen die dag vierhonderd mensen om het leven. Toen Shorouk onder het puin vandaan werd gehaald, bleek haar been verbrijzeld. Ze werd per ambulance naar het ziekenhuis gebracht, waar ze twaalf dagen moest wachten tot er mensen en middelen beschikbaar waren om haar te opereren. Pijnstillers waren niet voorhanden. Vervolgens werd het ziekenhuis waar zij lag gebombardeerd, waarna ze in een rolstoel naar het zuiden van Gaza vluchtte. Ten tijde van het interview woonde Shourouk met haar gezin in een tent in Rafah, zonder stroom of schoon water om haar ernstig geïnfecteerde wond te behandelen.
Met de uitrol van AI in conflictsituaties worden deze patronen van burgerleed in verstedelijkte gebieden op verschillende manieren versterkt. De Israëlische overheid vergaart al jaren data over de inwoners van Gaza en laat militairen daar het signature-strike-model op toepassen. De integratie van AI moet dit proces optimaliseren, maar de realiteit laat zien hoe onbetrouwbaar het gebruik van autonome gedragspatroonherkenning is en wat de gevolgen daarvan zijn in een complexe operationele context als Gaza. Zo zijn de patronen waarnaar het idf hun systemen in de data laat speuren zeer ruim gedefinieerd. In het geval van Lavendel lijkt een van de centrale regels zelfs 'man staat gelijk aan militant' te zijn. Het oprekken van wie of wat als legitiem doelwit wordt gezien, in combinatie met de hoge snelheid waarmee de systemen opereren, draagt bij aan de enorme schaal van verwoesting in Gaza.
Mensen in oorlogstijd passen hun gedrag aan. Zo geven burgers na bombardementen hun telefoon door aan familieleden, en veranderen vluchtelingen regelmatig van adres. Dit zijn precies de gedragspatronen die in Lavendel en Evangelie als 'verdacht' worden geprogrammeerd. Als burgers niet al onterecht door deze systemen worden geselecteerd, vergroot hun vluchtgedrag als reactie op het geweld de kans dat dit alsnog gebeurt.
Tot slot onthuldeThe New York Timesdat het Israëlische leger AI-gestuurde gezichtsherkenningstechnologie en Google Photo's is gaan gebruiken bij militaire controleposten in Gaza. Dit systeem 'herkende' de Palestijnse dichter en schrijver Mosab Abu Toha plus honderden andere burgers waarmee hij van Noord-naar Zuid-Gaza vluchtte als een Hamas-strijder. Ze werden door militairen staande gehouden en mishandeld. Uit het onderzoek vanThe New York Timesbleek dat het systeem hen ten onrechte als terrorist had geoormerkt en geïdentificeerd.
Veelvuldig wetenschappelijk onderzoek toont al jaren aan dat de integratie van AI-technologie in het veiligheidsdomein verre van feilloos is. Nu AI op grote schaal in oorlog wordt ingezet, zien we hoe problemen alsbiasen onnauwkeurigheid direct van invloed zijn op de levens van burgers en hun lichamelijke integriteit. Algoritmische oorlogsvoering vergroot niet alleen de schaal van burgerleed , maar leidt er bovendien toe dat burgers dag en nacht in een zekere 'psychische gevangenschap' verkeren, waarbij ze wel weten dat er op grote schaal data over hen worden verzameld, maar niet welk gedrag of fysiek kenmerk ze tot doelwit kan maken.
Terwijl burgerleed toeneemten nieuwe vormen aanneemt, wordt het antwoord op de vraag wie daarvoor verantwoordelijk en aansprakelijk is steeds complexer. Inlichtingenofficieren binnen het idf verschuilen zich achter de vermeende precisie en 'statistische' relevantie van de AI-systemen. Inlichtingenofficieren die met Lavendel hebben gewerkt, erkennen de beperkingen: 'Je kunt niet voor honderd procent op het systeem vertrouwen, zeker niet als het gaat om het koppelen van telefoonnummers aan personen.' Ook realiseren zij zich dat mensen in tijden van oorlog vaak van telefoon en adres veranderen. Toch baseren ze hun beslissingen grotendeels op deze systemen, met minimale menselijke controle. De Israëlische officieren die werden geïnterviewd, rechtvaardigden hun gebruik van het systeem door te verwijzen naar een handmatige controle die het idf aan het begin van de oorlog uitvoerde. Na twee weken zou het idf in een steekproef honderden van de door het systeem gegenereerde doelwitten hebben geverifieerd. Wij zetten grote vraagtekens bij de methode en betrouwbaarheid van deze verificatie, waarvan de details hoogstwaarschijnlijk voor altijd geheim zullen blijven.
De gerapporteerde negentig procent nauwkeurigheid heeft in de praktijk een blijvende impact. Fouten worden volgens een inlichtingenofficier 'statistisch benaderd': 'Gezien de omvang was het protocol dat je, ook al weet je niet zeker of het systeem gelijk heeft, statistisch gezien wel goed zit. Dus ga je ermee akkoord.' Een andere officier benadrukt zijn vertrouwen in het 'statistische mechanisme' en verwoordt daarmee ook de doelmatigheid van het systeem: 'De machine deed het op koelbloedige wijze en dat maakte het makkelijker.' Op deze manier wordt de verantwoordelijkheid voor een aanval van de mens gedelegeerd naar de technologie; die wordt, ondanks de bekende problemen en gerapporteerde foutmarge van tien procent, als objectief, neutraal en nauwkeurig beschouwd.
In een officiële reactie ontkende een woordvoerder van het idf de berichtgeving over Lavendel, Evangelie en Where's Daddy? met klem. Volgens de woordvoerder worden er weliswaar 'informatiebeheertools' ingezet om de inlichtingendiensten te ondersteunen bij het verzamelen en analyseren van informatie uit diverse bronnen, maar worden er geen AI-systemen gebruikt die specifiek gericht zijn op het identificeren van doelwitten.
Deze verklaring staat in schril contrast met andere, recentere berichtgeving vanThe Guardian. De Britse krant publiceerde een video waarin een hoge functionaris van de elite-inlichtingeneenheid 8200 van het idf vorig jaar sprak over het gebruik van machine-learning-algoritmen om terroristische doelen in Gaza te identificeren - wat hij beschreef als het 'magische poeder'.The Guardianwist bovendien te bevestigen dat de commandant van diezelfde eenheid in 2021 onder een pseudoniem beschreef hoe dergelijke AI-technologieën de 'menselijke bottleneck' bij het selecteren en goedkeuren van doelwitten zouden wegnemen.
De betrokken technologiebedrijven pareren de oplopende kritiek door te stellen dat zij geen invloed hebben op hoe eindgebruikers hun producten inzetten. 'Eenmaal geleverd, kunnen we niet controleren wat de klant ermee doet. Alles is mogelijk', aldus een Google-woordvoerder in reactie op vragen over Project Nimbus. Ondertussen groeit de onrust onder het Google-personeel en vreest een deel dat hun technologische innovaties 'de systematische discriminatie en verdringing van de Palestijnen alleen maar wreder en dodelijker zal maken'. Tientallen medewerkers die zich daar eerder tegen uitspraken zijn ontslagen, terwijl anderen uit protest ontslag namen. Google zelf blijft buiten schot.
Zowel de ontwikkelaars van militaire AI als de legers die de systemen inzetten, kunnen zich niet langer verschuilen achter de vermeende technologische superioriteit, precisie en neutraliteit van de technologie. De toekomst van AI-oorlogsvoering ontvouwt zich op dit moment. En de manier waarop deze systemen nu worden ontworpen, ingezet én bekritiseerd, bepaalt de koers voor morgen.
'Eigenlijk vuren we alleen op een mobiele telefoon', in de hoop daarmee de tegenstander te raken