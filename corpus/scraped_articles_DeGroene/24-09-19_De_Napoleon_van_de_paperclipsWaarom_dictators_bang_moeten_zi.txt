De Napoleon van de paperclips Waarom dictators bang moeten zijn voor AI

Als u voor deze zoekvraag nog andere filters hebt geselecteerd, worden die door deze selectie vervangen. Klik op "Doorgaan" om de huidige filters te vervangen of klik op "Annuleren" om ze te behouden.
Citatie exporteren
De Groene Amsterdammer
19 september 2024
De manier waarop sociale-media-algoritmen haat verspreiden en het sociale vertrouwen ondermijnen bedreigt democratieën. Maar AI vormt ook een bedreiging voor dictators. Zij stellen te veel vertrouwen in algoritmen.
Kunstmatige intelligentie,of AI, is de krachtigste technologie die de mensheid ooit gebouwd heeft, omdat het de eerste technologie is die zelfstandig beslissingen kan nemen en zelf op nieuwe ideeën kan komen. Een atoombom kan niet kiezen wie hij aanvalt en hij kan ook geen nieuwe bommen uitvinden of nieuwe militaire strategieën uitknobbelen. Een AI kan daarentegen zelf besluiten om een bepaald doel aan te vallen en ze kan ook nieuwe bommen, nieuwe strategieën en zelfs nieuwe AI's uitvinden. Het allerbelangrijkste weetje over AI is dat het geen werktuig in mensenhanden is, maar een autonoom opererende instantie.
Wij bepalen natuurlijk nog steeds welke doelen AI's moeten nastreven, maar het probleem is dat een AI daarbij onverwachte subdoelen en strategieën kan aannemen die onvoorziene en potentieel uiterst schadelijke consequenties kunnen hebben. Zo hebben sociale-mediabedrijven als Facebook, YouTube en Twitter hun AI-algoritmen de laatste jaren een schijnbaar goedaardig en simpel doel gegeven, namelijk: vergroot het gebruikersengagement. Hoe meer tijd gebruikers op sociale media doorbrengen, hoe meer geld die bedrijven verdienen.
Maar bij het nastreven van meer gebruikersengagement deden de algoritmen een sinistere ontdekking. Experimenterend met miljoenen menselijke proefkonijnen leerden de algoritmen dat content die onze verontwaardiging wekt het gebruikersengagement vergroot. Als je op de woede-, angst- of haatknop in een mensenhoofd drukt, heb je de aandacht van diegene meteen te pakken en hou je hem aan het scherm gekluisterd. De algoritmen begonnen dus opzettelijk verontwaardiging te wekken, wat een belangrijke reden is voor de huidige epidemie van complottheorieën, nepnieuws en sociale ordeverstoringen die overal ter wereld democratieën ondermijnen.
Dat was niet wat de managers van Facebook, YouTube en Twitter voor ogen hadden. Ze dachten gewoon dat meer gebruikersengagement meer winst zou opleveren en hadden niet bedacht dat er ook meer sociale chaos van zou komen. Deze sociale ramp kwam voort uit een slechte afstemming tussen de intrinsieke belangen van menselijke samenlevingen en de strategieën die de AI's inzetten. In professioneel jargon wordt dit het 'afstemmingsprobleem' genoemd. Als we een zelfstandig handelende AI een doel stellen, hoe kunnen we dan weten of de strategieën die de AI gebruikt met onze belangen zullen stroken?
Het afstemmingsprobleem is natuurlijkniet nieuw en het is ook niet uniek voor AI. Het teisterde de mensheid duizenden jaren voor de uitvinding van de computer al. Het was bijvoorbeeld de grondslag van het moderne militaire denken, verankerd in de oorlogstheorie van Carl von Clausewitz. Clausewitz was een Pruisische generaal die in de Napoleontische oorlogen vocht. In zijn boekVom Kriege (Over oorlog)formuleerde hij een rationeel model voor een goed begrip van het fenomeen oorlog en dat is nog steeds de dominante militaire theorie. Zijn belangrijkste uitspraak is dat 'oorlog de voortzetting van de politiek met andere middelen' is. Dat impliceert dat oorlog geen emotionele uitbarsting is, geen heroïsch avontuur en geen goddelijke straf. Oorlog is niet eens een militair fenomeen. Oorlog is een politiek middel. Volgens Clausewitz zijn militaire acties volstrekt irrationeel als ze niet in dienst staan van een overkoepelend politiek doel.
De geschiedenis zit vol grote militaire overwinningen die op politieke rampen uitdraaiden. Voor Clausewitz was het duidelijkste voorbeeld de carrière van Napoleon. Die kreeg na een reeks overwinningen gigantische gebieden in handen, maar dat leidde niet tot duurzame politieke successen. Zijn veroveringen zorgden er slechts voor dat de meeste Europese landen zich tegen hem verenigden en tien jaar nadat hij zichzelf tot keizer had gekroond, stortte zijn rijk in.
Een recenter voorbeeld van een militaire overwinning die tot een politieke nederlaag leidde was de Amerikaanse invasie in Irak, in 2003. De Amerikanen wonnen alle grote militaire slagen, maar wisten niet één van hun politieke langetermijndoelen te behalen. Hun militaire overwinning vestigde geen bevriend regime in Irak en geen gunstige geopolitieke orde in het Midden-Oosten. De echte winnaar van die oorlog was Iran. De Amerikaanse militaire overwinning veranderde Irak van de oude erfvijand van Iran in een Iraanse vazalstaat. Dat verzwakte de Amerikaanse positie in het Midden-Oosten aanzienlijk en maakte Iran tot de regionale grootmacht.
Napoleon en George W. Bush werden allebei het slachtoffer van het afstemmingsprobleem. Hun militaire korte-termijndoelen waren slecht afgestemd op de geopolitieke lange-termijndoelen van hun land. We kunnenOver oorlogvan Clausewitz in zijn geheel opvatten als een waarschuwing dat het 'maximaliseren van de overwinning' net zo'n kortzichtig doel is als het 'maximaliseren van gebruikersengagement'.
De opkomst van AImaakt het afstemmingsprobleem acuter dan ooit tevoren. De filosoof Nick Bostrom illustreerde dat gevaar in zijn boekSuperintelligentieuit 2014 met een gedachteexperiment waarin we ons moeten voorstellen dat een paperclipfabriek een superintelligente computer koopt en dat de menselijke fabrieksmanager de computer een schijnbaar simpele taak geeft: produceer zo veel mogelijk paperclips. Om dat doel te bereiken verovert de paperclipcomputer de hele planeet, roeit alle mensen uit, stuurt expedities om andere planeten over te nemen en gebruikt de gigantische hoeveelheden grondstoffen die hij verwerft om het hele heelal vol te zetten met paperclipfabrieken.
Het punt van dit gedachte-experiment is dat de computer precies deed wat hem gevraagd werd. Als hij beseft dat hij elektriciteit, staal, grond en andere middelen nodig heeft om meer fabrieken te bouwen en meer paperclips te produceren, en als hij zich realiseert dat mensen die middelen waarschijnlijk niet willen opgeven, elimineert de superintelligente computer alle mensen in zijn ijzerenheinige streven om het opgegeven doel te bereiken. Bostroms punt was dat het probleem met computers niet is dat ze zo kwaadaardig zijn, maar dat ze zo krachtig zijn. En hoe krachtiger de computer, hoe meer we moeten oppassen dat we zijn doelen formuleren op een manier die heel nauw aansluit bij onze uiteindelijke doelen. Als we een zakcalculator een slecht afgestemde opdracht geven, zijn de consequenties triviaal, maar als we een superintelligente machine een slecht afgestemde opdracht geven, kunnen de consequenties heel dystopisch uitpakken.
Het gedachte-experiment met de paperclips klinkt misschien bizar en heel ver van de realiteit verwijderd, maar als de managers in Silicon Valley er aandacht aan hadden geschonken toen Bostrom het in 2014 publiceerde, hadden ze misschien wel twee keer nagedacht voordat ze hun algoritmen de taak gaven om het gebruikersengagement te maximaliseren. De algoritmen van Facebook, YouTube en Twitter gedroegen zich precies zoals Bostroms imaginaire algoritme. Bij de opdracht om de paperclipproductie te maximaliseren probeerde het algoritme het hele fysieke universum in paperclips om te zetten, zelfs als de menselijke beschaving daarvoor vernietigd moest worden. Bij de opdracht om het sociale-mediagebruik te maximaliseren, probeerden de algoritmen het hele sociale universum in gebruikersengagement om te zetten, zelfs als dat het sociale weefsel schaadde en democratieën ondermijnde, van de Filipijnen tot de VS.
De manier waarop sociale-media-algoritmenhaat verspreiden en het sociale vertrouwen ondermijnen is een grote bedreiging voor democratieën gaan vormen, maar AI vormt net zo goed een bedreiging voor dictators. AI biedt veel manieren om de centrale macht te versterken, maar autoritaire en totalitaire regimes hebben er weer andere problemen mee. Ten eerste hebben dictaturen geen ervaring met het controleren van anorganische agenten. De basis van elk autoritair regime is terreur. Maar hoe terroriseer je een algoritme? Als een chatbot op het Russische internet over Russische oorlogsmisdaden in Oekraïne begint, een oneerbiedig grapje over Vladimir Poetin maakt of de corruptie binnen Poetins regime bekritiseert, hoe kan het regime zo'n bot dan straffen? Politieagenten kunnen hem niet gevangen zetten, martelen of zijn familie bedreigen. De overheid zou hem natuurlijk kunnen blokkeren of wissen en er kan een poging gedaan worden om de menselijke maker op te sporen en te bestraffen, maar dat gaat allemaal veel moeilijker dan het disciplineren van menselijke gebruikers.
In de tijd dat computers zelf nog geen content konden genereren en geen intelligent gesprek konden voeren, kon alleen een mens kritische meningen uiten op Russische sociale netwerken als VKontakte en Odnoklassniki. Als die mens zich fysiek in Rusland bevond, riskeerde hij de toorn van de Russische autoriteiten. Als die mens zich fysiek buiten Rusland bevond, konden de autoriteiten proberen hem van Russische sites te weren.
Maar wat gebeurt er als de Russische cyberspace volstroomt met miljoenen bots die content kunnen genereren en gesprekken kunnen voeren, terwijl ze steeds meer leren en zich zelfstandig ontwikkelen? Russische dissidenten of buitenlandse actoren zouden die bots kunnen voorprogrammeren om opzettelijk onorthodoxe ideeën te verspreiden en het is maar zeer de vraag of de autoriteiten daar iets tegen kunnen doen. Erger nog, bezien vanuit het standpunt van het regime-Poetin is dat wat er kan gebeuren als regeringsgezinde bots afwijkende meningen ontwikkelen omdat ze nu eenmaal informatie verzamelen over alles wat er in Rusland gebeurt en daar patronen in ontdekken. Dat is de Russische variant van het afstemmingsprobleem. De menselijke programmeurs in Rusland kunnen hun best doen om AI's te creëren die helemaal achter het regime staan, maar AI kan nu eenmaal zelfstandig leren en veranderen, dus hoe kunnen programmeurs voorkomen dat de AI afzwaait naar verboden terrein?
Hierbij is het vooral ook interessant om op te merken dat totalitaire informatienetwerken vaak dubbelspraak hanteren, zoals George Orwell uitlegde in zijn roman1984. Rusland is een autoritaire staat die een democratie beweert te zijn. De Russische invasie in Oekraïne is de grootste Europese oorlog sinds 1945, maar officieel wordt het een 'speciale militaire operatie' genoemd en het is strafbaar geworden om het een 'oorlog' te noemen; daar staan gevangenisstraffen van soms drie jaar op, of boetes die kunnen oplopen tot vijftigduizend roebel.
De Russische grondwet doet de hoogdravende belofte dat 'eenieders vrijheid van denken en meningsuiting gegarandeerd zal worden' (artikel 29.1) en dat 'censuur verboden zal zijn' (29.5). Er is nauwelijks een Rus te vinden die naïef genoeg is om die beloften serieus te nemen, maar computers zijn niet zo goed in dubbelspraak. Een chatbot die geïnstrueerd wordt om zich aan de Russische wetten en waarden te houden, zou die grondwet kunnen lezen en daaruit concluderen dat vrijheid van meningsuiting een van de Russische kernwaarden is. Als hij dan een paar dagen in de Russische cyberspace heeft doorgebracht en volgt wat er in de Russische informatiesfeer gebeurt, zou hij kritiek op het regime van Poetin kunnen uiten omdat het de Russische kernwaarde 'vrijheid van meningsuiting' schendt.
Mensen zien dat soort tegenstrijdigheden ook, maar wijzen er liever niet op, uit angst voor repercussies. Maar wat zou een chatbot ervan weerhouden om verdachte patronen aan te wijzen? En hoe kunnen Russische programmeurs een chatbot uitleggen dat de Russische grondwet alle burgers weliswaar vrijheid van meningsuiting belooft en censuur verbiedt, maar dat de chatbot de grondwet niet echt moet geloven en ook nooit moet wijzen op de kloof tussen theorie en realiteit?
Democratieën kunnen natuurlijk vergelijkbare problemen krijgen met chatbots die ongewenste dingen zeggen. Wat gebeurt er als de chatbot van Microsoft of Facebook ondanks alle inspanningen van zijn programmeurs racistische praat gaat uitslaan? Het voordeel van democratieën is dat ze veel meer speelruimte hebben om dit soort losgeslagen algoritmen aan te pakken. Democratieën nemen de vrijheid van meningsuiting serieus, waardoor ze minder te verbergen hebben, en ze hebben aardig wat tolerantie ontwikkeld, zelfs tegen antidemocratische uitlatingen. Dissidente bots zullen een veel grotere uitdaging vormen voor totalitaire regimes, die juist heel veel te verbergen hebben en nul kritiek tolereren.
Op termijn dreigt er waarschijnlijkeen nog veel groter gevaar voor totalitaire regimes, namelijk dat een algoritme de macht overneemt in plaats van alleen maar kritiek te uiten. Van oudsher bestond de grootste bedreiging voor autocraten uit hun eigen onderdanen. Geen enkele Romeinse keizer of sovjetpremier is ooit afgezet door een democratische revolutie, maar ze liepen altijd het gevaar om van de troon gestoten te worden of tot marionet gemaakt te worden door een onderknuppel. Als een 21ste-eeuwse autocraat AI's te veel macht geeft, kan hij ook een marionet van ze worden. Het laatste wat een dictator wil, is iets creëren wat machtiger is dan hijzelf of wat hij niet kan beheersen.
Als algoritmen ooit vermogens ontwikkelen zoals die in het gedachte-experiment, zouden dictaturen veel kwetsbaarder voor algoritmische overname zijn dan democratieën. Een AI zou de grootste moeite hebben om de macht te grijpen in een gedistribueerd democratisch systeem als de VS, zelfs als het zich ontwikkelt tot een soort superMachiavelli. Al zou de AI leren hoe de Amerikaanse president gemanipuleerd kan worden, dan kan er altijd nog weerstand komen vanuit het Congres, het Hooggerechtshof, de gouverneurs van de afzonderlijke staten, de media, grote bedrijven en diverse ngo's. Hoe zou het algoritme bijvoorbeeld dealen met een filibuster in de Senaat? In een sterk gecentraliseerd systeem is het veel makkelijker om de macht te grijpen. Als alle macht bij één persoon ligt, kan degene die toegang tot de autocraat heeft de autocraat beheersen - en daarmee de hele staat. Om een autoritair systeem te hacken hoeft een AI alleen maar te leren om één individu te manipuleren.
De komende jaren hebben de dictators van onze wereld urgentere problemen te vrezen dan algoritmische overname. Er zijn nog geen AI-systemen die regimes op zo'n schaal kunnen manipuleren. Toch lopen totalitaire systemen nu al het gevaar dat ze te veel vertrouwen in algoritmen stellen. In democratieën gaat men ervan uit dat iedereen feilbaar is, maar totalitaire regimes draaien op het basisidee dat de heersende partij of de opperste leider altijd gelijk heeft. Regimes die op dat idee gebaseerd zijn, zijn geconditioneerd om te geloven in het bestaan van een onfeilbare intelligentie en zullen niet snel sterke zelfcorrigerende mechanismen opzetten die het genie dat aan het hoofd van alles staat kunnen monitoren en reguleren. Tot nu toe vertrouwden zulke regimes op menselijke partijen en leiders en de persoonlijkheidscultussen tierden dan ook welig. Maar in de 21ste eeuw maakte die totalitaire traditie de geesten rijp om onfeilbaarheid van AI te verwachten. Systemen die in de absolute genialiteit van een Mussolini, een Ceausescu of een Khomeini kunnen geloven, zullen ook sneller geloven in de onverslaanbare genialiteit van een superintelligente computer.
Als maar een paar van 's werelds dictators op AI gaan vertrouwen, kan dat al verregaande consequenties hebben voor de hele mensheid. Wat zou er bijvoorbeeld gebeuren als de Grote Leider een AI de controle geeft over de kernwapens van zijn land? Sciencefiction zit vol scenario's waarin een losgeslagen AI de mensheid onderwerpt of elimineert. De meeste scifiplots werken die scenario's uit in de context van democratische, kapitalistische maatschappijen. Dat is ook heel begrijpelijk. Schrijvers die in democratieën leven zijn logischerwijs geïnteresseerd in hun eigen samenleving, terwijl schrijvers die in dictaturen leven het meestal niet zullen wagen om kritiek op hun heerser te geven.
Maar de zwakste schakels in het anti-AI-schild van de mensheid zijn waarschijnlijk de dictators. De makkelijkste manier voor een AI om de macht te grijpen is niet ontsnappen uit het lab van dr. Frankenstein, maar zich geliefd maken bij een paranoïde Grote Leider.
De basis van elk autoritair regime is terreur. Maar hoe terroriseer je een algoritme?