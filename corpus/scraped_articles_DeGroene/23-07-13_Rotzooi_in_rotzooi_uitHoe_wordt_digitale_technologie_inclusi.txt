Rotzooi in, rotzooi uit Hoe wordt digitale technologie inclusiever?

Als u voor deze zoekvraag nog andere filters hebt geselecteerd, worden die door deze selectie vervangen. Klik op "Doorgaan" om de huidige filters te vervangen of klik op "Annuleren" om ze te behouden.
Citatie exporteren
De Groene Amsterdammer
13 juli 2023
Wat je in zelflerende AI-systemen stopt, krijg je terug. Technologie, veelal ontwikkeld door witte mannen, versterkt en verbergt daardoor de vooroordelen. Met name vrouwen (van kleur) luiden de alarmbel.
'Not all speed ismovement.' Ruha Benjamin begint met een quote van activist en schrijver Toni Cade Bambara. Socioloog Benjamin opent de conferentie For a Collective Internet, waarop wordt nagedacht over hoe een eerlijker internet eruit kan zien. Ze noemt twee toekomstbeelden die we vaak voorgeschoteld krijgen als het over technologie gaat. 'De ene is techno-dystopisch: technologie gaat ons uitroeien. De tweede is het techno-utopische verhaal: technologie zal ons redden, zal alles eerlijker maken, efficiënter.' En hoewel die twee tegenovergesteld lijken, delen ze volgens Benjamin onderliggende logica: 'In beide zijn de mensen achter de schermen van de technologie onzichtbaar. En daarmee hun waarden en ideologieën die in onze digitale wereld gecodeerd worden. Dus het eerste wat we moeten doen, is die onthullen.'
Het gesprek over technologie en kunstmatige intelligentie gaat vaak over doemscenario's. Na de aanvankelijke opwinding over alle mogelijkheden die generatieve Artificiële Intelligentie (AI), zoals chatbot ChatGPT, met zich meebracht, kwamen zorgen over of we de machine die we gemaakt hebben nog wel in de hand hebben. Prominenten als mede-oprichter van Apple Steve Wozniak, Elon Musk en historicus Yuval Harari, en met hen andere tech-ceo's, professoren en onderzoekers waarschuwden in maart van dit jaar dat de technologie ons boven het hoofd dreigt te groeien. In een open brief stellen ze een pauze voor, 'een stap terug doen van de gevaarlijke race naar steeds grotere, onvoorspelbare black-box-modellen'. Volgens de brief bevinden we ons momenteel in een 'AI-zomer', maar moeten we voorkomen dat we in de herfst terechtkomen. Ondertussen klinkt er ook een ander geluid, zij het meestal zachter. Dat vertelt ons dat we helemaal niet in een AI-zomer zitten.
Oumaima Hajri spreekt, ondanks het vroege tijdstip, helder en weloverwogen. Ik spreek haar voor ze naar het ministerie van Binnenlandse Zaken vertrekt voor een sessie over de invloed van generatieve AI: systemen die zelf tekst, afbeeldingen of audio genereren. 'Er is in de politiek nog steeds veel te weinig kennis over de ethische impact.' Hajri is docent-onderzoeker AI, Ethiek en Samenleving aan de Hogeschool van Rotterdam. Daar leert ze jonge programmeurs kritisch te reflecteren op de systemen die ze bouwen. Daarnaast doet ze aan de Universiteit van Cambridge zelf onderzoek naar de sociaal-maatschappelijke impact van AI. Die is groot, volgens haar: 'AI zorgt er vaak voor dat achtergestelde groepen verder gemarginaliseerd raken: vluchtelingen worden ermee opgespoord en teruggestuurd, in de mijnen waar grondstoffen voor de technologie gedolven worden is sprake van kinderarbeid, en arbeiders in bijvoorbeeld Kenia raken getraumatiseerd omdat ze de hele dag content moeten labelen en de vreselijkste dingen zien.' En die ongelijkheid is ook verweven in de AI zelf, ziet ze.
Machine learning-systemenzijn onderhevig aan wat in computerwetenschappen ook wel het 'rotzooi in, rotzooi uit'-principe wordt genoemd. Wat je erin stopt, krijg je terug. Dat betekent dat ook vooroordelen, racisme, seksisme en validisme, homofobie, transfobie en islamofobie ingebed raken in het systeem, omdat ze ingesleten zitten in de brondata. Vraag AI naar een plaatje van een verpleger (in het Engels het onzijdigenurse)en je krijgt een vrouw, vraag het naar een plaatje van een ceo en je krijgt voornamelijk witte mannen. Dat is niet de 'fout' van de AI, het is simpelweg een representatie van wat er online te vinden is, wat weer een afspiegeling is van hoe er in de maatschappij over verplegers of ceo's gedacht wordt.
Dit is geen nieuw inzicht. Computerwetenschapper Timnit Gebru was leidinggevende in het Ethical AI-team van Google toen ze in 2021 samen met anderen een paper publiceerde over de gevaren van vertekening in brondata, wat leidt tot 'oververtegenwoordigen van hegemonische standpunten en het coderen van vooroordelen die mogelijk schadelijk zijn voor gemarginaliseerde bevolkingsgroepen'. Google eiste dat Gebru het paper introk, wat ze weigerde. Uiteindelijk leidde het tot haar vertrek bij de techgigant. Volgens het bedrijf haar eigen besluit, maar volgens Gebru werd ze weggestuurd.
Vertekende brondata hebben ook gevolgen voor hoe AI-toepassingen en software werken. De lijst voorbeelden is lang. Gezichtsherkenning werkt slechter bij mensen van kleur dan bij witte mensen, zoalsCoded Biasvan Shalini Kantayya laat zien. De documentaire volgt Joy Buolamwini, een zwarte vrouw die gezichtsherkenning in een universiteitsproject wil gebruiken, maar erachter komt dat die alleen werkt wanneer ze een wit masker voor haar gezicht houdt. Automatische zeepdispensers werken vaak niet bij handen met een donkere huidskleur. Zuurstofmeters in het ziekenhuis ook niet, en geven daarom onjuist aan dat het zuurstofgehalte in het bloed in orde is. Deze systemen zijn gemaakt door en vaak getest op witte personen. 'Witheid wordt gezien als de onzichtbare standaard voor gebruikers van softwaresystemen', schrijft datajournalist Meredith Broussard.
Hajri merkte vroeger zelf al dat het systeem niet voor haar ontworpen was, vertelt ze. 'Ik heb het uitgeprobeerd. Als ik een sollicitatiebrief ondertekende met een westerse naam werd ik uitgenodigd op gesprek, als ik mijn eigen naam eronder zette niet.' Dit soort vooroordelen worden nu overgenomen en uitvergroot in AI-systemen. Of, zoals Buolamwini het verwoordt inCoded Bias:'Data is onze geschiedenis. Het verleden woont in onze algoritmen.'
Technologie versterkt enverbergt de vooroordelen. We zijn bereid toe te geven dat mensen fouten maken, maar zijn geneigd te denken dat computers neutraal en onfeilbaar zijn. Broussard haalt in haar recente boekMore Than a Glitchhet voorbeeld aan van Robert Julian-Borchak Williams, die door gezichtsherkenningssoftware van de politie werd gekoppeld aan een winkeldiefstal en voor zijn huis werd klemgereden. De agenten die hem arresteerden moesten nadat Williams een nacht in de cel had doorgebracht toegeven dat Williams absoluut niet leek op de man die door beveiligingscamera's in de winkel was vastgelegd. Iets wat het menselijk oog meteen zag, maar de software niet, omdat die veel slechter is in het herkennen van gezichten van personen van kleur.
Broussard beschrijft de grenzen van de intelligentie met een analogie van een koektrommel. Zij en haar broertje vochten vroeger om het laatste koekje. Uiteindelijk besloten ze te delen, maar een kinderhand breekt een koekje niet precies door de helft. De ruzie zette zich voort om de grootste helft, tot een van de twee een acceptabel aanbod deed: ik krijg de grootste helft, dan mag jij kiezen wat we op tv gaan kijken. Dat is een oplossing die AI niet kan bedenken. Weliswaar biedt ChatGPT dit als een mogelijke oplossing aan, zo blijkt wanneer je dit dilemma aan de chatbot voorlegt, maar bepalen wát van even grote waarde kan zijn als het koekje, volgens sociale in plaats van wiskundige logica, dat kan de bot niet.
Het beeld van AI als onbegrijpelijke black box klopt dan ook niet, schrijft ze.De wiskunde die dit soort modellen gebruikt is weliswaar zeer complex en moeilijk om over te praten, maar 'het is nietonmogelijkom te beschrijven'.
'Die black box bestaat niet', zegt ook Hajri stellig. 'We kunnen in hoge mate begrijpen hoe AI werkt. Bij AI is er zogenaamdeexplainability,we kunnen uitleggen hoe het systeem technisch werkt. Maar precies interpreteren hoe het tot een bepaalde uitkomst komt, dat kunnen we niet. Dat is het verschil tussen AI en mensen; aan een mens kun je altijd vragen wat de afwegingen waren. Bij een zelflerend systeem kun je dat niet helemaal doorgronden. Maar dat betekent niet dat je moet zeggen: "We begrijpen het niet, we kunnen er niks aan veranderen." Dat is gevaarlijk.'
MensenrechtenadvocaatNani Jansen Reventlow verblijft momenteel in Denemarken. Achter haar in beeld is het bos te zien waarin haar zomerhuisje staat. Haar zinnen zijn doorspekt met Engels, aangezien ze al jaren voornamelijk buiten Nederland werkt. Ze staat regelmatig in de rechtszaal bij grote internationale mensenrechtenzaken. Ze woonde tot voor kort in Berlijn, waar ze vijf jaar geleden aan de wieg stond van het Digital Freedom Fund (DFF). Het fonds wil digitale rechten voor iedereen beschermen en handhaven, en is daarom begonnen met het oor breed te luisteren te leggen. Dat was een idee van Jansen Reventlow. 'Ik zag een groepsfoto van een van de eerste bijeenkomsten met betrokkenen die we organiseerden en ik besefte dat ik de enige niet-witte persoon was. Ik ging erop letten en zag dat veel gemarginaliseerde groepen niet gehoord worden omdat ze niet vertegenwoordigd zijn, en de weg naar officiële inspraakmogelijkheden niet kennen bij bijvoorbeeld nieuwe wetgeving. Er werd vooral erg veel gepraat over diversiteit en inclusie, maar weinig gedaan.'
Door met deze groepen in gesprek te gaan, brengt het fonds in kaart hoe hun digitale rechten beter beschermd kunnen worden. Ook subsidieert het fonds rechtszaken, bijvoorbeeld over over platformwerkers in Amsterdam bij bedrijven als Uber, algoritmen bij visumaanvragen in het VK die oneerlijk zouden zijn, en om inzage af te dwingen in algoritmen die door de overheid van Polen worden gebruikt. Na vijf jaar droeg Reventlow het DFF over en richtte Systemic Justice op, een organisatie die voortbouwt op het werk van het DFF en zelf namens verschillende gemeenschappen ook rechtszaken voert. Jansen Reventlow vindt de focus op toekomstige dreiging voor het voortbestaan van de mensheid frustrerend. 'Die alarmistische framing spreekt in de media natuurlijk erg aan. Er zit nu al iets fundamenteel fout in de systemen, maar de stemmen die dat over het voetlicht proberen te brengen worden nauwelijks gehoord. Terwijl er mensen zijn die al schade ondervinden: je hoeft niet te wachten opkiller robotsom levens te vernietigen, dat kan ook met een simpel algoritme in een spreadsheet.' Dat ziet Hajri ook. 'Niet iedereen heeft de luxe om over de toekomst na te denken, omdat ze tegen dagelijkse problemen aan het vechten zijn. Daar ligt ook een privilege: wie bepaalt waar we over gaan praten, robots die ons gaan overnemen of impact nu?'
Wat opvalt in de stemmendie wél over de huidige problemen in technologie praten: het zijn voornamelijk vrouwen en vaak vrouwen van kleur. Dat is een contrast met wie bij grote tech-bedrijven Google, Amazon, Apple en Meta de dienst uitmaken: zij zijn grotendeels mannelijk en wit.
Voor Nani Jansen Reventlow wringt die observatie. 'Het verheerlijkt op een bepaalde manier dat mensen dit uit noodzaak doen. De reden dat gemarginaliseerde groepen zoals vrouwen van kleur vaker aan de alarmbel trekken is simpelweg omdat zij eerder de consequenties ondervinden. Tegelijkertijd wordt er vervolgens lange tijd niet naar hen geluisterd, totdat het wordt overgenomen door de witte mainstream. Ik vind het dan wrang om te zeggen: wat fantastisch dat zij dit al zo vroeg zagen.' Ze is even stil. 'Ik haat het ook als mensen het hebben over"giving a voice to the voiceless".We hebben allemaal een stem, maar er wordt heel selectief geluisterd.'
'Het is weleens moeilijk dat wat je bepleit in je werk ook over jezelf gaat', zegt Hajri erover. 'Als je kritiek krijgt, is het ook pijnlijker. Als ik zelf als kritisch wordt gelabeld omdat ik dit soort dingen aankaart, denk ik: het zijn gewoon feiten. Ik ben er gepassioneerd over omdat dit mij raakt, maar daar heb ik ook niet voor gekozen. Iemand zei laatst tegen me: "Het moet wel een leuk verhaal blijven." Ik was echt ﬂabbergasted.'
Het is erg moeilijk om een kritische stem op dit gebied te vinden die niet vrouwelijk is, constateert adviseur technologie en mensenrechten Tanya O'Carroll. 'Terwijl, als je kijkt naar wie zo'n open brief ondertekenen, dan zijn het toch vooral witte mannen. Als je zelf de consequenties doormaakt, kijk je anders naar problemen die er zijn. Misschien is dat minder interessant voor de jongens met macht in Silicon Valley, zij kunnen lekkernerdenover hypothetische gevaren van de toekomst. Bovendien is er voor hen een groot winstbelang om niet te veel in te zetten op regulatie nú.'
Is de angst voor AI die de wereld overneemt dan niet reëel? Nee, zegt Hajri. 'We hebben er feitelijk nog weinig van gezien. Ik vind veel van de scienceﬁction-scenario's echt om te lachen. Het lijkt me meer marketing vanuit Big Tech om de aandacht af te leiden van de huidige problemen.'
Samen met de roep om voorzichtigheid klinkt ook de roep om de controle vooral bij een handvol dominante tech-bedrijven te houden, zegt O'Carroll. 'De timing is geen toeval, met nieuwe en strengere wetgeving die er in Europa aan komt. Deze bedrijven kiezen ervoor om nu hun motorkap op te lichten en mensen bang te maken. Ik maak me zorgen dat dat de aandacht snel aﬂeidt van de weg die we waren ingeslagen, richting een grotere rol voor de staat in regulering en handhaving.'
O'Carroll was mede-oprichtervan de technologie-tak van Amnesty International en coördineert het netwerk People vs. Big Tech. Ze voert sinds november vorig jaar een rechtszaak tegen Meta, het moederbedrijf van Facebook. 'Toen ik zwanger was, werd ik gebombardeerd met baby- en moederschap-advertenties. Dat wilde ik niet.'
Facebook geeft zijn gebruikers allerlei labels. Die informatie is beschikbaar voor adverteerders, op basis waarvan gebruikers advertenties te zien krijgen. Het zijn heel speciﬁeke labels, zoals 'Net verloofd' (keuze uit drie, zes of twaalf maanden). 'Ik zag bij mijn labels "moederschap", maar ook heel persoonlijke informatie zoals mijn politieke voorkeur, interesse in "homoseksualiteit" en "feminisme". Facebook geeft je zogenaamd veel vrijheid om dit zelf aan te passen.' Maar toen O'Carroll het label 'moederschap' verwijderde, kwamen er andere labels voor in de plaats, zoals 'geboorte' of 'baby's'. Ze kreeg dezelfde advertenties. 'Ik heb Facebook gevraagd te stoppen met mij te proﬁleren en met het doorspelen van mijn informatie. Dat weigerden ze. Toen ben ik op basis van de GDPR, de Europese variant van de AVG-wetgeving, naar de rechter gestapt.'
Als O'Carroll wint, kan dat grote gevolgen hebben voor wat Meta en andere bedrijven nog met gebruikersgegevens mogen doen. 'Ze hebben enorm veel informatie over je. Er waren zevenhonderd labels aan mij gekoppeld. Een journalist die de rechtszaak versloeg zei dat hij het gevoel had een veel te intiem inkijkje in mijn leven te hebben gehad nadat hij ze had doorgenomen.'
De gevolgen blijven niet bij een irritante reclame. 'In het verleden is al gebleken dat Facebook door adverteerders gebruikt werd om advertenties voor leningen of huizen niet aan mensen te laten zien op basis van etniciteit, of dat vrouwen geen advertenties voor bepaalde banen te zien krijgen', vertelt O'Carroll. 'Volgens de AVG-wetgeving mag Facebook 'gevoelige' categorieën als huidskleur, religie of seksualiteit helemaal niet gebruiken. Inmiddels hebben ze er daarom de meest gevoelige geschrapt. Maar ik wil de optie dat er helemaal geen labels gebruikt worden.'
Een van de reactiesdie onderzoeker Timnit Gebru bij Google kreeg toen ze de problemen in AI en technologie aan de kaak probeerde te stellen, was dat wetenschappers en ontwerpers het niet hun verantwoordelijkheid vinden om na te denken over ethische en morele dilemma's achter de technieken die ze ontwikkelen. 'Het idee dat je tech ontwikkelt in een soort vacuüm is op de lange termijn gewoon niet houdbaar', zegt Jansen Reventlow. 'Ik ben hoopvol dat de huidige generatie zich bewust is van deze discussies en vragen stelt bij wat ze maken.'
Dat is precies wat Oumaima Hajri haar studenten probeert bij te brengen. 'Hierover nadenken is niet gemakkelijk, vooral als je anders geschoold bent. Als studenten binnenkomen zijn ze in shock dat ze moeten gaan ﬁlosoferen. Maar je vervult als programmeur en engineer ook een maatschappelijke taak.' Met haar studenten denkt ze vanaf het begin van het ontwerpproces na over de ethische implicaties. 'Is deze technologie wel de oplossing voor het probleem? Hoe kunnen we in het ontwerp ook rekening houden met de meest gemarginaliseerde groepen? We lossen de meest complexe sociaal-maatschappelijke problemen graag op met technologie, maar daar loop je dus juist tegen discriminatie en racisme aan.'
Alle drie kijken zenaar wetgeving en handhaving voor een betere toekomst. O'Carroll: 'Ik ben voorzichtig optimistisch over de Digital Services Act.' Deze DSA is een nieuwe wet van de Europese Commissie om digitale rechten beter te beschermen. 'Het laat politieke wil zien, dat is een eerste stap. Maar handhaving is de echte test. Onafhankelijke onderzoeken en testmethodes moeten de standaard worden, net zoals bedrijven accountants hebben. Nu worden de regels vooral gemaakt door bedrijven zelf, die dat graag zo houden.'
'Fijn dat het mogelijk wordt om je te verzetten tegen het schenden van je rechten', zegt Jansen Reventlow over de wet. 'Aan de andere kant, de verantwoordelijkheid ligt daarmee opnieuw bij individuen, die zich moeten kunnen organiseren, een advocaat kunnen betalen, enzovoorts. Ik hoop niet dat het een aﬂeidingsmanoeuvre van toezicht en handhaving zal worden.'
'Het is hetzelfde als met klimaatverandering, maar dan lopen we tien jaar achter', zegt O'Carroll. 'We horen alleen: AI is ingewikkeld. Overheden moeten niet te snel handelen, anders hangt daar een groot prijskaartje aan. Vroeger hoorden we hetzelfde over klimaatverandering. Nu weten we het allemaal en wordt in elke krant geschreven over de klimaatcrisis.'
'Dat het ingewikkeld is, is een handig argument om kritische pottenkijkers buiten de deur te houden', zegt Jansen Reventlow. 'Je hoeft niet te weten hoe een auto exact werkt om te weten dat je verkeersregels nodig hebt.'